# Experiment 2: Lower learning rate for fine-tuning
# LR: 5e-5 (4x lower than baseline)
# Hypothesis: Lower LR prevents disrupting pretrained weights

name: exp2_lower_lr

model:
  num_features: 48
  upscale: 4

data:
  manga109_dir: data/Manga109_released_2023_12_07/images
  seed: 42

training:
  batch_size: 16
  max_iters: 5000
  lr: 0.00005          # 5e-5 (4x lower)
  min_lr: 0.000001
  weight_decay: 0.01
  val_interval: 500
  patience: 1000

loss:
  type: L1
  use_perceptual: false

pretrained:
  path: checkpoints/SPAN_4x_pretrained.pth

output:
  checkpoint_dir: checkpoints/exp2_lower_lr
  results_dir: outputs/exp2_lower_lr

